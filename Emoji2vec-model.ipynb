{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "9d432840",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import emoji\n",
    "import nltk.tokenize as tk\n",
    "import gensim as gsm\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "from contraction_map import contraction_map as cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "8b2be4cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fc4abea8930>"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "SEED = 2022\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c590c3",
   "metadata": {},
   "source": [
    "## Data Loading & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "7448c690",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('Data/emoji2vec_data/emoji2vec_train.xlsx')[['content', 'label']]\n",
    "test = pd.read_excel('Data/emoji2vec_data/emoji2vec_test.xlsx')[['content', 'label']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3daf0d",
   "metadata": {},
   "source": [
    "### Data cleaning functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "051e8f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference: https://www.kaggle.com/code/stoicstatic/twitter-sentiment-analysis-using-word2vec-bilstm \n",
    "\n",
    "urlPattern        = r\"((http://)[^ ]*|(https://)[^ ]*|(www\\.)[^ ]*)\"\n",
    "userPattern       = '@[^\\s]+'\n",
    "hashtagPattern    = '#[^\\s]+'\n",
    "alphaPattern      = \"[^a-z0-9<>]\"\n",
    "sequencePattern   = r\"(.)\\1\\1+\"\n",
    "seqReplacePattern = r\"\\1\\1\"\n",
    "\n",
    "def is_alnum_or_emoji_or_space(char):\n",
    "    return char.isalnum() or emoji.is_emoji(char) or char in ('\\t', ' ')\n",
    "\n",
    "def preprocess_apply(tweet):\n",
    "\n",
    "    tweet = tweet.lower()\n",
    "\n",
    "    # Replace all URls with '<url>'\n",
    "    tweet = re.sub(urlPattern,'<url>',tweet)\n",
    "    # Replace @USERNAME to '<user>'.\n",
    "    tweet = re.sub(userPattern,'<user>', tweet)\n",
    "    \n",
    "    # Replace 3 or more consecutive letters by 2 letter.\n",
    "    tweet = re.sub(sequencePattern, seqReplacePattern, tweet)\n",
    "\n",
    "    for contraction, replacement in cm.CONTRACTION_MAP.items():\n",
    "        tweet = tweet.replace(contraction, replacement)\n",
    "\n",
    "    # Remove non-alphanumeric and symbols\n",
    "    tweet = ''.join(filter(is_alnum_or_emoji_or_space, tweet))\n",
    "\n",
    "    # Adding space on either side of '/' to seperate words (After replacing URLS).\n",
    "    tweet = re.sub(r'/', ' / ', tweet)\n",
    "    return tweet\n",
    "\n",
    "# End of reference. The following code is wrote by me.\n",
    "\n",
    "def emoji2description(text):  \n",
    "    return emoji.replace_emoji(text, replace=lambda chars, data_dict: ' '.join(data_dict['en'].split('_')).strip(':'))\n",
    "\n",
    "def emoji2concat_description(text):\n",
    "    emoji_list = emoji.emoji_list(text)\n",
    "    ret = emoji.replace_emoji(text, replace='').strip()\n",
    "    for json in emoji_list:\n",
    "        this_desc = ' '.join(emoji.EMOJI_DATA[json['emoji']]['en'].split('_')).strip(':')\n",
    "        ret += ' ' + this_desc\n",
    "    return ret\n",
    "\n",
    "def extract_emojis(text):\n",
    "    emoji_list = emoji.emoji_list(text)\n",
    "#     print(emoji_list)\n",
    "    ret = []\n",
    "    for json in emoji_list:\n",
    "        this_emoji = json['emoji']\n",
    "        ret.append(this_emoji)\n",
    "    return ' '.join(ret)\n",
    "\n",
    "def keep_only_emojis(data):\n",
    "    cnt = data['content'].apply(emoji.emoji_count)\n",
    "    return data[cnt >= 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "171afe9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of train data is (41343,)\n",
      "shape of test data is (12920,)\n"
     ]
    }
   ],
   "source": [
    "data['cleaned_content'] = data.content.apply(preprocess_apply)\n",
    "test['cleaned_content'] = test.content.apply(preprocess_apply)\n",
    "X,y = data['cleaned_content'].values, pd.get_dummies(data['label']).values\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=SEED, test_size=0.2)\n",
    "X_test, y_test = test['cleaned_content'].values, pd.get_dummies(test['label']).values\n",
    "print(f'shape of train data is {X_train.shape}')\n",
    "print(f'shape of test data is {X_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "db11805c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetDataset(Dataset):\n",
    "\n",
    "    def __init__(self, tweets, targets, tokenizer, max_len, e2v, w2v):\n",
    "        self.tweets = tweets\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.e2v = e2v\n",
    "        self.w2v = w2v\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tweets)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        tweet = str(self.tweets[item])\n",
    "        target = self.targets[item]\n",
    "\n",
    "        tokens = self.tokenizer.tokenize(tweet)\n",
    "        \n",
    "        seq = []\n",
    "        for t in tokens:\n",
    "            if t in e2v.key_to_index:\n",
    "                seq.append(torch.from_numpy(e2v[t]))\n",
    "            elif t in w2v.key_to_index:\n",
    "                seq.append(torch.from_numpy(w2v[t]))\n",
    "        \n",
    "        \n",
    "        padding_length = self.max_len - len(seq)\n",
    "        for _ in range(padding_length):\n",
    "            seq.append(torch.zeros(300,))\n",
    "        seq = torch.stack(seq, dim=0)\n",
    "        \n",
    "        return seq\n",
    "\n",
    "def create_data_loader(X, y, tokenizer, max_len, batch_size, e2v, w2v):\n",
    "    ds = TweetDataset(\n",
    "    tweets=X,\n",
    "    targets=y,\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=max_len,\n",
    "    e2v=e2v,\n",
    "    w2v=w2v\n",
    "    )\n",
    "\n",
    "    return DataLoader(\n",
    "    ds,\n",
    "    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcd7051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the word2vec and emoji2vec models\n",
    "e2v_path = 'Data/emoji2vec_data/emoji2vec.bin'\n",
    "w2v_path = 'Data/emoji2vec_data/GoogleNews-vectors-negative300.bin.gz'\n",
    "w2v = gsm.models.KeyedVectors.load_word2vec_format(w2v_path, binary=True)\n",
    "e2v = gsm.models.KeyedVectors.load_word2vec_format(e2v_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "eca1a6dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample batch shape: torch.Size([64, 128, 300])\n"
     ]
    }
   ],
   "source": [
    "TweetTknzr = tk.TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=True)\n",
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_data_loader = create_data_loader(X_train, y_train, TweetTknzr, MAX_LEN, BATCH_SIZE, e2v, w2v)\n",
    "val_data_loader = create_data_loader(X_val, y_val, TweetTknzr, MAX_LEN, BATCH_SIZE, e2v, w2v)\n",
    "\n",
    "dataiter = iter(train_data_loader)\n",
    "sample = dataiter.next()\n",
    "print(\"Sample batch shape:\", sample.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3b5df6",
   "metadata": {},
   "source": [
    "## Neural Network Building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2339de",
   "metadata": {},
   "source": [
    "## Training & Evaluating"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
