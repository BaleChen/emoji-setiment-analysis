{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install openpyxl","metadata":{"execution":{"iopub.status.busy":"2022-07-20T10:52:48.392401Z","iopub.execute_input":"2022-07-20T10:52:48.393381Z","iopub.status.idle":"2022-07-20T10:52:57.554304Z","shell.execute_reply.started":"2022-07-20T10:52:48.393343Z","shell.execute_reply":"2022-07-20T10:52:57.553204Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"!pip install openpyxl","metadata":{"execution":{"iopub.status.busy":"2022-07-20T12:01:41.969250Z","iopub.execute_input":"2022-07-20T12:01:41.970153Z","iopub.status.idle":"2022-07-20T12:01:54.955581Z","shell.execute_reply.started":"2022-07-20T12:01:41.969980Z","shell.execute_reply":"2022-07-20T12:01:54.954424Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom nltk.corpus import stopwords \nfrom collections import Counter\nimport string\nimport emoji\nimport re\nimport seaborn as sns\nfrom tqdm import tqdm\nfrom collections import defaultdict\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import TensorDataset, DataLoader, Dataset\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2022-07-20T12:01:54.957831Z","iopub.execute_input":"2022-07-20T12:01:54.958126Z","iopub.status.idle":"2022-07-20T12:01:57.306255Z","shell.execute_reply.started":"2022-07-20T12:01:54.958099Z","shell.execute_reply":"2022-07-20T12:01:57.305253Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"data = pd.read_excel('../input/semeval2015task11/SemEval-2015-Scraped.xlsx')[['content', 'label']]\ntest = pd.read_excel('../input/semeval15-task11-trial-data/SemEval-2015-trial-data.xlsx')[['content', 'label']]","metadata":{"execution":{"iopub.status.busy":"2022-07-20T12:01:57.307717Z","iopub.execute_input":"2022-07-20T12:01:57.308386Z","iopub.status.idle":"2022-07-20T12:01:58.579097Z","shell.execute_reply.started":"2022-07-20T12:01:57.308338Z","shell.execute_reply":"2022-07-20T12:01:58.577885Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"test.plot.kde()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-07-19T05:56:06.473919Z","iopub.execute_input":"2022-07-19T05:56:06.475087Z","iopub.status.idle":"2022-07-19T05:56:06.679503Z","shell.execute_reply.started":"2022-07-19T05:56:06.475030Z","shell.execute_reply":"2022-07-19T05:56:06.678549Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"labels = test.label.to_numpy()\nprint(np.mean(labels),np.std(labels))","metadata":{"execution":{"iopub.status.busy":"2022-07-19T05:56:12.386259Z","iopub.execute_input":"2022-07-19T05:56:12.387297Z","iopub.status.idle":"2022-07-19T05:56:12.395303Z","shell.execute_reply.started":"2022-07-19T05:56:12.387246Z","shell.execute_reply":"2022-07-19T05:56:12.393215Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"import torch\nimport random\n\nSEED = 2022\n\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)","metadata":{"execution":{"iopub.status.busy":"2022-07-20T12:01:58.581556Z","iopub.execute_input":"2022-07-20T12:01:58.581924Z","iopub.status.idle":"2022-07-20T12:01:58.596195Z","shell.execute_reply.started":"2022-07-20T12:01:58.581885Z","shell.execute_reply":"2022-07-20T12:01:58.595361Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def emoji2description(text):  \n    return emoji.replace_emoji(text, replace=lambda chars, data_dict: ' '.join(data_dict['en'].split('_')).strip(':'))\n\ndef emoji2concat_description(text):\n    emoji_list = emoji.emoji_list(text)\n    ret = emoji.replace_emoji(text, replace='').strip()\n    for json in emoji_list:\n        this_desc = ' '.join(emoji.EMOJI_DATA[json['emoji']]['en'].split('_')).strip(':')\n        ret += ' ' + this_desc\n    return ret\n\ndef extract_emojis(text):\n    emoji_list = emoji.emoji_list(text)\n#     print(emoji_list)\n    ret = []\n    for json in emoji_list:\n        this_emoji = json['emoji']\n        ret.append(this_emoji)\n    return ' '.join(ret)","metadata":{"execution":{"iopub.status.busy":"2022-07-20T12:03:34.913438Z","iopub.execute_input":"2022-07-20T12:03:34.914030Z","iopub.status.idle":"2022-07-20T12:03:34.922332Z","shell.execute_reply.started":"2022-07-20T12:03:34.913993Z","shell.execute_reply":"2022-07-20T12:03:34.921069Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"extract_emojis(\"I love ðŸ˜„ it ðŸ˜” ðŸ’—\")","metadata":{"execution":{"iopub.status.busy":"2022-07-20T10:57:58.237077Z","iopub.execute_input":"2022-07-20T10:57:58.237417Z","iopub.status.idle":"2022-07-20T10:57:58.248400Z","shell.execute_reply.started":"2022-07-20T10:57:58.237388Z","shell.execute_reply":"2022-07-20T10:57:58.247309Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"from transformers import BertTokenizer, BertModel","metadata":{"execution":{"iopub.execute_input":"2022-07-14T03:13:42.645464Z","iopub.status.busy":"2022-07-14T03:13:42.644651Z","iopub.status.idle":"2022-07-14T03:13:43.282401Z","shell.execute_reply":"2022-07-14T03:13:43.281388Z","shell.execute_reply.started":"2022-07-14T03:13:42.645420Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModel","metadata":{"execution":{"iopub.status.busy":"2022-07-20T12:01:58.608476Z","iopub.execute_input":"2022-07-20T12:01:58.609699Z","iopub.status.idle":"2022-07-20T12:01:58.988684Z","shell.execute_reply.started":"2022-07-20T12:01:58.609661Z","shell.execute_reply":"2022-07-20T12:01:58.987700Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')","metadata":{"execution":{"iopub.execute_input":"2022-07-14T04:57:05.443473Z","iopub.status.busy":"2022-07-14T04:57:05.443139Z","iopub.status.idle":"2022-07-14T04:57:07.154016Z","shell.execute_reply":"2022-07-14T04:57:07.153065Z","shell.execute_reply.started":"2022-07-14T04:57:05.443444Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer.decode(tokenizer.encode('I love it ðŸ˜„ ðŸ˜” ðŸ’—'))","metadata":{"execution":{"iopub.execute_input":"2022-07-14T04:57:11.650185Z","iopub.status.busy":"2022-07-14T04:57:11.649767Z","iopub.status.idle":"2022-07-14T04:57:11.661124Z","shell.execute_reply":"2022-07-14T04:57:11.660159Z","shell.execute_reply.started":"2022-07-14T04:57:11.650151Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')","metadata":{"execution":{"iopub.status.busy":"2022-07-20T12:01:58.990273Z","iopub.execute_input":"2022-07-20T12:01:58.990812Z","iopub.status.idle":"2022-07-20T12:02:15.945973Z","shell.execute_reply.started":"2022-07-20T12:01:58.990776Z","shell.execute_reply":"2022-07-20T12:02:15.944938Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"tokenizer.decode(tokenizer.encode('I love it ðŸ˜„ ðŸ˜” ðŸ’—'))","metadata":{"execution":{"iopub.status.busy":"2022-07-20T12:02:15.947501Z","iopub.execute_input":"2022-07-20T12:02:15.947862Z","iopub.status.idle":"2022-07-20T12:02:20.826500Z","shell.execute_reply.started":"2022-07-20T12:02:15.947827Z","shell.execute_reply":"2022-07-20T12:02:20.825467Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Load the special tokens and configurations\ninit_token = tokenizer.cls_token\neos_token = tokenizer.sep_token\npad_token = tokenizer.pad_token\nunk_token = tokenizer.unk_token\n\ninit_token_idx = tokenizer.cls_token_id\neos_token_idx = tokenizer.sep_token_id\npad_token_idx = tokenizer.pad_token_id\nunk_token_idx = tokenizer.unk_token_id\n\nmax_input_length = tokenizer.max_model_input_sizes['xlm-roberta-base']\n","metadata":{"execution":{"iopub.status.busy":"2022-07-20T12:02:20.829228Z","iopub.execute_input":"2022-07-20T12:02:20.830568Z","iopub.status.idle":"2022-07-20T12:02:20.837286Z","shell.execute_reply.started":"2022-07-20T12:02:20.830527Z","shell.execute_reply":"2022-07-20T12:02:20.836301Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def tokenize_and_cut(sentence):\n    tokens = tokenizer.tokenize(sentence) \n    tokens = tokens[:max_input_length-2]\n    return tokens","metadata":{"execution":{"iopub.status.busy":"2022-07-20T12:02:20.839269Z","iopub.execute_input":"2022-07-20T12:02:20.840047Z","iopub.status.idle":"2022-07-20T12:02:20.856583Z","shell.execute_reply.started":"2022-07-20T12:02:20.840009Z","shell.execute_reply":"2022-07-20T12:02:20.855482Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def keep_only_emojis(data):\n    cnt = data['content'].apply(emoji.emoji_count)\n    return data[cnt >= 1]","metadata":{"execution":{"iopub.status.busy":"2022-07-20T12:11:08.437725Z","iopub.execute_input":"2022-07-20T12:11:08.438087Z","iopub.status.idle":"2022-07-20T12:11:08.442861Z","shell.execute_reply.started":"2022-07-20T12:11:08.438055Z","shell.execute_reply":"2022-07-20T12:11:08.441888Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"data = keep_only_emojis(data)\ntest = keep_only_emojis(test)","metadata":{"execution":{"iopub.status.busy":"2022-07-20T12:12:43.196730Z","iopub.execute_input":"2022-07-20T12:12:43.197331Z","iopub.status.idle":"2022-07-20T12:12:43.606184Z","shell.execute_reply.started":"2022-07-20T12:12:43.197292Z","shell.execute_reply":"2022-07-20T12:12:43.605198Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"# X,y = data['content'].values, data['label'].values\n# X_train, X_test, y_train, y_test = train_test_split(X, y)\nX_train, y_train = data['content'].values, data['label'].values\nX_test, y_test = test['content'].values, test['label'].values\nprint(f'shape of train data is {X_train.shape}')\nprint(f'shape of test data is {X_test.shape}')","metadata":{"execution":{"iopub.status.busy":"2022-07-20T12:12:45.701424Z","iopub.execute_input":"2022-07-20T12:12:45.701772Z","iopub.status.idle":"2022-07-20T12:12:45.708936Z","shell.execute_reply.started":"2022-07-20T12:12:45.701743Z","shell.execute_reply":"2022-07-20T12:12:45.707661Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"dd = pd.Series(y_train).value_counts()\nsns.barplot(x=np.array(['negative','positive']),y=dd.values)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-19T04:43:14.274476Z","iopub.execute_input":"2022-07-19T04:43:14.274837Z","iopub.status.idle":"2022-07-19T04:43:14.320420Z","shell.execute_reply.started":"2022-07-19T04:43:14.274798Z","shell.execute_reply":"2022-07-19T04:43:14.318931Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"def preprocess_string(s):\n    # This cleans out all emojis!!!\n    # Remove all non-word characters (everything except numbers and letters)\n    s = re.sub(r\"[^\\w\\s]\", '', s)\n    # Replace all runs of whitespaces with no space\n    s = re.sub(r\"\\s+\", '', s)\n    # replace digits with no space\n    s = re.sub(r\"\\d\", '', s)\n\n    return s\n\ndef tokenize(x_train,y_train,x_val,y_val):\n#     word_list = []\n\n#     stop_words = set(stopwords.words('english')) \n#     for sent in x_train:\n#         for word in sent.lower().split():\n#             word = preprocess_string(word)\n#             if word not in stop_words and word != '':\n#                 word_list.append(word)\n  \n#     corpus = Counter(word_list)\n#     # sorting on the basis of most common words\n#     corpus_ = sorted(corpus,key=corpus.get,reverse=True)[:1000]\n#     # creating a dict\n#     onehot_dict = {w:i+1 for i,w in enumerate(corpus_)}\n    \n#     # tockenize\n#     final_list_train,final_list_test = [],[]\n#     for sent in x_train:\n#             final_list_train.append([onehot_dict[preprocess_string(word)] for word in sent.lower().split() \n#                                      if preprocess_string(word) in onehot_dict.keys()])\n#     for sent in x_val:\n#             final_list_test.append([onehot_dict[preprocess_string(word)] for word in sent.lower().split() \n#                                     if preprocess_string(word) in onehot_dict.keys()])\n    final_list_train, final_list_test = [], []\n    for sent_1 in x_train:\n        final_list_train = tokenizer(preprocess_string(sent_1))\n    for sent_2 in x_val:\n        final_list_test = tokenizer(preprocess_string(sent_2))\n        \n    encoded_train = [1 if label == True else 0 for label in y_train]  \n    encoded_test = [1 if label == True else 0 for label in y_val]\n    \n    return np.array(final_list_train), np.array(encoded_train),np.array(final_list_test), np.array(encoded_test)","metadata":{"execution":{"iopub.execute_input":"2022-07-14T04:59:41.379581Z","iopub.status.busy":"2022-07-14T04:59:41.379027Z","iopub.status.idle":"2022-07-14T04:59:41.390499Z","shell.execute_reply":"2022-07-14T04:59:41.389425Z","shell.execute_reply.started":"2022-07-14T04:59:41.379544Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"token_lens = []\n\nfor txt in data['content']:\n    tokens = tokenizer.encode(txt, max_length=512)\n    token_lens.append(len(tokens))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TweetDataset(Dataset):\n\n    def __init__(self, tweets, targets, tokenizer, max_len):\n        self.tweets = tweets\n        self.targets = targets\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.tweets)\n\n    def __getitem__(self, item):\n        tweets = extract_emojis(str(self.tweets[item]))\n        target = self.targets[item]\n\n        encoding = self.tokenizer.encode_plus(\n          tweets,\n          add_special_tokens=True,\n          max_length=self.max_len,\n          return_token_type_ids=False,\n          padding='max_length',\n          return_attention_mask=True,\n          return_tensors='pt',\n        )\n\n        return {\n          'tweet_text': tweets,\n          'input_ids': encoding['input_ids'].flatten(),\n          'attention_mask': encoding['attention_mask'].flatten(),\n          'targets': torch.tensor(target, dtype=torch.float32)\n        }","metadata":{"execution":{"iopub.status.busy":"2022-07-20T12:02:43.971989Z","iopub.execute_input":"2022-07-20T12:02:43.973060Z","iopub.status.idle":"2022-07-20T12:02:43.984083Z","shell.execute_reply.started":"2022-07-20T12:02:43.973021Z","shell.execute_reply":"2022-07-20T12:02:43.983222Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def create_data_loader(X, y, tokenizer, max_len, batch_size):\n    ds = TweetDataset(\n    tweets=X,\n    targets=y,\n    tokenizer=tokenizer,\n    max_len=max_len\n    )\n\n    return DataLoader(\n    ds,\n    batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2022-07-20T12:02:47.990014Z","iopub.execute_input":"2022-07-20T12:02:47.990498Z","iopub.status.idle":"2022-07-20T12:02:48.002263Z","shell.execute_reply.started":"2022-07-20T12:02:47.990453Z","shell.execute_reply":"2022-07-20T12:02:48.000150Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 16\nMAX_LEN = 128\n\ntrain_data_loader = create_data_loader(X_train, y_train, tokenizer, MAX_LEN, BATCH_SIZE)\ntest_data_loader = create_data_loader(X_test, y_test, tokenizer, MAX_LEN, BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2022-07-20T12:12:59.201347Z","iopub.execute_input":"2022-07-20T12:12:59.202019Z","iopub.status.idle":"2022-07-20T12:12:59.207827Z","shell.execute_reply.started":"2022-07-20T12:12:59.201979Z","shell.execute_reply":"2022-07-20T12:12:59.206382Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"len(train_data_loader)","metadata":{"execution":{"iopub.status.busy":"2022-07-20T12:13:02.480787Z","iopub.execute_input":"2022-07-20T12:13:02.481132Z","iopub.status.idle":"2022-07-20T12:13:02.487147Z","shell.execute_reply.started":"2022-07-20T12:13:02.481102Z","shell.execute_reply":"2022-07-20T12:13:02.486148Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"dataiter = iter(train_data_loader)\nsample = dataiter.next()\nprint(sample.keys())\nprint(sample['input_ids'].shape)\nprint(sample['attention_mask'].shape)\nprint(sample['targets'].shape)","metadata":{"execution":{"iopub.status.busy":"2022-07-20T12:13:04.804922Z","iopub.execute_input":"2022-07-20T12:13:04.805266Z","iopub.status.idle":"2022-07-20T12:13:04.820097Z","shell.execute_reply.started":"2022-07-20T12:13:04.805238Z","shell.execute_reply":"2022-07-20T12:13:04.818891Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"code","source":"bert = AutoModel.from_pretrained('xlm-roberta-base')","metadata":{"execution":{"iopub.status.busy":"2022-07-20T12:03:44.571752Z","iopub.execute_input":"2022-07-20T12:03:44.572103Z","iopub.status.idle":"2022-07-20T12:04:45.105761Z","shell.execute_reply.started":"2022-07-20T12:03:44.572072Z","shell.execute_reply":"2022-07-20T12:04:45.104856Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"class BERT_BiLSTM_FFF_Cls(nn.Module):\n\n    def __init__(self, bert, hidden_dim, bidirectional):\n        super(BERT_BiLSTM_FFF_Cls, self).__init__()\n        embedding_dim = bert.config.to_dict()['hidden_size']\n        self.hidden_dim = hidden_dim\n        self.bert = bert\n        self.bidirectional = bidirectional\n        \n        self.lstm = nn.LSTM(input_size=embedding_dim,\n                           hidden_size=self.hidden_dim,\n                           num_layers=2,\n                           bidirectional=self.bidirectional,\n                           batch_first=True)\n        \n        self.drop = nn.Dropout(p=0.25)\n        \n        self.out = nn.Sequential(\n                    nn.Linear(self.hidden_dim * 4, 512),\n                    nn.ReLU(),\n                    nn.Linear(512, 1)\n                   )\n\n    def forward(self, input_ids, attention_mask):\n        with torch.no_grad():\n            embeddings = self.bert(\n              input_ids=input_ids,\n              attention_mask=attention_mask\n            )[0]\n        \n        last_hidden = self.lstm(embeddings)[1][0]\n        \n        #hidden = [n layers * n directions, batch size, emb dim]\n        \n        last_hidden = self.drop(torch.flatten(last_hidden.transpose(0,1), start_dim=1, end_dim=2))\n        \n        return self.out(last_hidden).ravel()","metadata":{"execution":{"iopub.status.busy":"2022-07-20T12:04:54.943536Z","iopub.execute_input":"2022-07-20T12:04:54.944492Z","iopub.status.idle":"2022-07-20T12:04:54.955459Z","shell.execute_reply.started":"2022-07-20T12:04:54.944437Z","shell.execute_reply":"2022-07-20T12:04:54.954066Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"hidden_dim = 512\nbidirectional = True\nEPOCHS = 20\nlr=0.001\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(device)\n\nbaseline_1 = BERT_BiLSTM_FFF_Cls(bert, hidden_dim, bidirectional)\n\nloss_fn = nn.MSELoss()\noptimizer = torch.optim.Adam(baseline_1.parameters(), lr=lr)\n","metadata":{"execution":{"iopub.status.busy":"2022-07-20T12:13:10.496707Z","iopub.execute_input":"2022-07-20T12:13:10.497376Z","iopub.status.idle":"2022-07-20T12:13:10.611532Z","shell.execute_reply.started":"2022-07-20T12:13:10.497336Z","shell.execute_reply":"2022-07-20T12:13:10.610440Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"## Training ","metadata":{}},{"cell_type":"code","source":"def train_epoch(\n  model, \n  data_loader, \n  loss_fn, \n  optimizer, \n  device, \n  n_examples\n):\n    model = model.train()\n\n    losses = []\n    correct_predictions = 0\n\n    for d in tqdm(data_loader):\n        input_ids = d[\"input_ids\"].to(device)\n        attention_mask = d[\"attention_mask\"].to(device)\n        targets = d[\"targets\"].to(device)\n\n        outputs = model(\n                input_ids=input_ids,\n                attention_mask=attention_mask\n                )\n#         print(outputs.dtype)\n#         _,preds = torch.max(outputs, dim = 1)\n        loss = loss_fn(outputs, targets)\n#         prediction_error += torch.sum(torch.abs(targets - outputs))\n#         correct_predictions += torch.sum(preds == torch.max(targets, dim = 1)[1])\n#         print(f'Iteration loss: {loss.item()}')\n        losses.append(loss.item())\n\n        loss.backward()\n        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n        optimizer.zero_grad()\n\n#     return correct_predictions.double() / n_examples, np.mean(losses)\n    return np.mean(losses), np.mean(losses)","metadata":{"execution":{"iopub.status.busy":"2022-07-20T12:04:59.122965Z","iopub.execute_input":"2022-07-20T12:04:59.123986Z","iopub.status.idle":"2022-07-20T12:04:59.169595Z","shell.execute_reply.started":"2022-07-20T12:04:59.123932Z","shell.execute_reply":"2022-07-20T12:04:59.168455Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"def eval_model(model, data_loader, loss_fn, device, n_examples):\n    model = model.eval()\n    \n    losses = []\n    correct_predictions = 0\n    \n    output_all = torch.tensor([])\n    target_all = torch.tensor([])\n    output_all, target_all = output_all.to(device), target_all.to(device)\n    with torch.no_grad():\n        for d in data_loader:\n            \n            \n            input_ids = d[\"input_ids\"].to(device)\n            attention_mask = d[\"attention_mask\"].to(device)\n            targets = d[\"targets\"].to(device)\n            \n            outputs = model(\n            input_ids=input_ids,\n            attention_mask=attention_mask\n            )\n            target_all = torch.cat([target_all,targets])\n            output_all = torch.cat([output_all,outputs])\n#             _, preds = torch.max(outputs, dim=1)\n#             prediction_error += torch.sum(torch.abs(targets - outputs))\n            loss = loss_fn(outputs, targets)\n            \n#             correct_predictions += torch.sum(preds == torch.max(targets, dim = 1)[1])\n            losses.append(loss.item())\n#     return correct_predictions.double() / n_examples, np.mean(losses)\n    return nn.functional.cosine_similarity(output_all, target_all, dim=0), np.mean(losses)","metadata":{"execution":{"iopub.status.busy":"2022-07-20T12:40:59.412030Z","iopub.execute_input":"2022-07-20T12:40:59.412385Z","iopub.status.idle":"2022-07-20T12:40:59.423668Z","shell.execute_reply.started":"2022-07-20T12:40:59.412355Z","shell.execute_reply":"2022-07-20T12:40:59.422489Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"history = defaultdict(list)\nbest_accuracy = 0\nbaseline_1.to(device)\n\nfor epoch in range(EPOCHS):\n\n    print(f'Epoch {epoch + 1}/{EPOCHS}')\n    print('-' * 10)\n\n    train_acc, train_loss = train_epoch(\n    baseline_1,\n    train_data_loader,    \n    loss_fn, \n    optimizer, \n    device,\n    len(X_train)\n    )\n\n    print(f'Train loss {train_loss} accuracy {train_acc}')\n\n    val_acc, val_loss = eval_model(\n    baseline_1,\n    test_data_loader,\n    loss_fn, \n    device, \n    len(X_test)\n    )\n\n    print(f'Val   loss {val_loss} cos sim {val_acc}')\n    print()\n\n    history['train_acc'].append(train_acc)\n    history['train_loss'].append(train_loss)\n    history['val_acc'].append(val_acc)\n    history['val_loss'].append(val_loss)\n\n    if val_acc > best_accuracy:\n        torch.save(baseline_1.state_dict(), 'best_model_state.bin')\n        best_accuracy = val_acc","metadata":{"execution":{"iopub.status.busy":"2022-07-20T12:41:01.614705Z","iopub.execute_input":"2022-07-20T12:41:01.615077Z","iopub.status.idle":"2022-07-20T12:43:06.249693Z","shell.execute_reply.started":"2022-07-20T12:41:01.615047Z","shell.execute_reply":"2022-07-20T12:43:06.248079Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"torch.tensor([])","metadata":{"execution":{"iopub.status.busy":"2022-07-20T12:29:15.464585Z","iopub.execute_input":"2022-07-20T12:29:15.465818Z","iopub.status.idle":"2022-07-20T12:29:15.473705Z","shell.execute_reply.started":"2022-07-20T12:29:15.465775Z","shell.execute_reply":"2022-07-20T12:29:15.472559Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"np.sum((y_test+1.7570397111913356)**2)/len(y_test)","metadata":{"execution":{"iopub.status.busy":"2022-07-19T05:56:31.358867Z","iopub.execute_input":"2022-07-19T05:56:31.360217Z","iopub.status.idle":"2022-07-19T05:56:31.368653Z","shell.execute_reply.started":"2022-07-19T05:56:31.360166Z","shell.execute_reply":"2022-07-19T05:56:31.367654Z"},"trusted":true},"execution_count":10,"outputs":[]}]}